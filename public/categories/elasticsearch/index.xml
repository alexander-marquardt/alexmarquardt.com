<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Elasticsearch on Alexander Marquardt</title>
    <link>http://localhost:1313/categories/elasticsearch/</link>
    <description>Recent content in Elasticsearch on Alexander Marquardt</description>
    <generator>Hugo -- 0.153.4</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 06 Nov 2020 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/elasticsearch/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Using Elasticsearch Painless scripting to recursively iterate through JSON fields</title>
      <link>http://localhost:1313/posts/using-elasticsearch-painless-scripting-to-iterate-through-fields/</link>
      <pubDate>Fri, 06 Nov 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/using-elasticsearch-painless-scripting-to-iterate-through-fields/</guid>
      <description>&lt;h2 id=&#34;authors&#34;&gt;Authors&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Alexander Marquardt&lt;/li&gt;
&lt;li&gt;Honza Kral&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-scripting-painless.html&#34;&gt;Painless&lt;/a&gt;&lt;/em&gt; is a simple, secure scripting language designed specifically for use with Elasticsearch. It is the default scripting language for Elasticsearch and can safely be used for inline and stored scripts. In one of its many use cases, Painless can modify documents as they are ingested into your Elasticsearch cluster. In this use case, you may find that you would like to use Painless to evaluate every field in each document that is received by Elasticsearch. However, because of the hierarchical nature of JSON documents, how to iterate over all of the fields may be non-obvious.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Understanding and fixing &#34;too many script compilations&#34; errors in Elasticsearch</title>
      <link>http://localhost:1313/posts/elasticsearch-too-many-script-compilations/</link>
      <pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/elasticsearch-too-many-script-compilations/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;When using Elasticsearch, in some rare instances you may see an error such as &amp;ldquo;Too many dynamic script compilations within X minutes&amp;rdquo;. Such an error may be caused by a poor script design &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-scripting-using.html#prefer-params&#34;&gt;where parameters are hard-coded&lt;/a&gt;. In other cases this may be due to the script cache being too small or the compilation limit being too low. In this article, I will show how to determine if these default limits are too low, and how these limits can be modified.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Converting CSV to JSON in Filebeat</title>
      <link>http://localhost:1313/posts/converting-csv-to-json-in-filebeat/</link>
      <pubDate>Tue, 17 Mar 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/converting-csv-to-json-in-filebeat/</guid>
      <description>&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Many organisations use excel files for creating and storing important data. For various reasons it may be useful to import such data into Elasticsearch. For example, one may need to get &lt;a href=&#34;https://en.wikipedia.org/wiki/Master_data&#34;&gt;Master Data&lt;/a&gt; that is created in a spreadsheet into Elasticsearch where it could be used for &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/7.x/ingest-enriching-data.html&#34;&gt;enriching Elasticsearch documents&lt;/a&gt;. Or one may wish to use Elasticsearch and Kibana for analysing a dataset that is only available in a spreadsheet. In such cases, one option is to use &lt;a href=&#34;https://www.elastic.co/guide/en/beats/filebeat/7.6/filebeat-overview.html&#34;&gt;Filebeat&lt;/a&gt; for uploading such CSV data into an Elasticsearch cluster.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Using Logstash and Elasticsearch scripted upserts to transform eCommerce purchasing data</title>
      <link>http://localhost:1313/posts/logstash-and-elasticsearch-painless-scripted-upserts-transform-data/</link>
      <pubDate>Tue, 17 Dec 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/logstash-and-elasticsearch-painless-scripted-upserts-transform-data/</guid>
      <description>&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/logstash/7.5/introduction.html&#34;&gt;Logstash&lt;/a&gt; is a tool that can be used to collect, process, and forward events to Elasticsearch. In order to demonstrate the power of Logstash when used in conjunction with Elasticsearch’s &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/7.5/docs-update.html#scripted_upsert&#34;&gt;scripted upserts&lt;/a&gt;, I will show you how to create a near-real-time &lt;a href=&#34;https://www.elastic.co/elasticon/2015/sf/building-entity-centric-indexes&#34;&gt;entity-centric index&lt;/a&gt;. Once data is transformed into an entity-centric index, many kinds of analysis become possible with simple (cheap) queries rather than more computationally intensive aggregations.&lt;/p&gt;
&lt;p&gt;As a note, using the approach demonstrated here would result in documents similar to those generated by &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/7.5/transforms.html&#34;&gt;Elasticsearch transforms&lt;/a&gt;. Nevertheless, the technique that is documented has not been benchmarked against Elasticsearch transforms, as the main goal of this blog is to demonstrate the power and flexibility of Logstash combined with scripted upserts.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Emulating transactional functionality in Elasticsearch with two-phase commits</title>
      <link>http://localhost:1313/posts/emulating-transactional-functionality-in-elasticsearch-with-two-phase-commits/</link>
      <pubDate>Thu, 05 Dec 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/emulating-transactional-functionality-in-elasticsearch-with-two-phase-commits/</guid>
      <description>&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Elasticsearch supports atomic create, update, and delete operations at the individual document level, but does not have built-in &lt;a href=&#34;https://www.elastic.co/blog/found-elasticsearch-as-nosql#transactions&#34;&gt;support for multi-document transactions&lt;/a&gt;. Although Elasticsearch does not position itself as a system of record for storing data, in some cases it may be necessary to modify multiple documents as a single cohesive unit. Therefore, in this blog post we present a &lt;a href=&#34;https://en.wikipedia.org/wiki/Two-phase_commit_protocol&#34;&gt;two-phase commit protocol&lt;/a&gt; which can be used to &lt;em&gt;emulate&lt;/em&gt; multi-document transactions.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Converting local time to ISO 8601 time in Elasticsearch</title>
      <link>http://localhost:1313/posts/converting-local-time-to-iso-8601-time-in-elasticsearch/</link>
      <pubDate>Wed, 16 Oct 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/converting-local-time-to-iso-8601-time-in-elasticsearch/</guid>
      <description>&lt;p&gt;This article is available at: &lt;a href=&#34;https://www.elastic.co/blog/converting-local-time-to-iso-8601-time-in-elasticsearch&#34;&gt;https://www.elastic.co/blog/converting-local-time-to-iso-8601-time-in-elasticsearch&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Counting unique beats agents sending data into Elasticsearch</title>
      <link>http://localhost:1313/posts/counting-unique-beats-agents-elasticsearch/</link>
      <pubDate>Thu, 18 Jul 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/counting-unique-beats-agents-elasticsearch/</guid>
      <description>&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;When using Beats with Elasticsearch, it may be useful to keep track of how many unique agents are sending data into an Elasticsearch cluster, and how many documents each agent is submitting. Such information for example could be useful for detecting if beats agents are behaving as expected.&lt;/p&gt;
&lt;p&gt;In this blog post, I first discuss how to efficiently specify a filter for documents corresponding to a particular time range, followed by several methods for detecting how many beats agents are sending documents to Elasticsearch within the specified time range.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Debugging Elasticsearch and Lucene with IntelliJ IDEA</title>
      <link>http://localhost:1313/posts/debugging-elasticsearch-and-lucene-with-intellij-idea/</link>
      <pubDate>Sat, 02 Feb 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/debugging-elasticsearch-and-lucene-with-intellij-idea/</guid>
      <description>&lt;p&gt;This article can be found at: &lt;a href=&#34;https://www.elastic.co/blog/how-to-debug-elasticsearch-source-code-in-intellij-idea&#34;&gt;https://www.elastic.co/blog/how-to-debug-elasticsearch-source-code-in-intellij-idea&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>A step-by-step guide to enabling security, TLS/SSL, and PKI authentication in Elasticsearch</title>
      <link>http://localhost:1313/posts/security-tls-ssl-pki-authentication-in-elasticsearch/</link>
      <pubDate>Mon, 05 Nov 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/security-tls-ssl-pki-authentication-in-elasticsearch/</guid>
      <description>&lt;p&gt;This article is available at: &lt;a href=&#34;https://www.elastic.co/blog/elasticsearch-security-configure-tls-ssl-pki-authentication&#34;&gt;https://www.elastic.co/blog/elasticsearch-security-configure-tls-ssl-pki-authentication&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Using Logstash to drive filtered data from a single source into multiple output destinations</title>
      <link>http://localhost:1313/posts/using-logstash-to-drive-filtered-data-from-a-single-source-into-multiple-output-destinations/</link>
      <pubDate>Fri, 31 Aug 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/using-logstash-to-drive-filtered-data-from-a-single-source-into-multiple-output-destinations/</guid>
      <description>&lt;p&gt;This article is available at: &lt;a href=&#34;https://www.elastic.co/blog/using-logstash-to-split-data-and-send-it-to-multiple-outputs&#34;&gt;https://www.elastic.co/blog/using-logstash-to-split-data-and-send-it-to-multiple-outputs&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Using Logstash prune capabilities to whitelist sub-documents</title>
      <link>http://localhost:1313/posts/using-logstash-prune-capabilities-to-whitelist-sub-documents/</link>
      <pubDate>Tue, 28 Aug 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/using-logstash-prune-capabilities-to-whitelist-sub-documents/</guid>
      <description>&lt;h1 id=&#34;overview&#34;&gt;Overview&lt;/h1&gt;
&lt;p&gt;Logstash&amp;rsquo;s &lt;a href=&#34;https://www.elastic.co/guide/en/logstash/current/plugins-filters-prune.html&#34;&gt;prune filter plugin&lt;/a&gt; can make use of whitelists to ensure that only specific desired fields are output from Logstash, and that all other fields are dropped. In this blog post we demonstrate the use of Logstash to whitelist desired fields and desired sub-documents before indexing into Elasticsearch.&lt;/p&gt;
&lt;h1 id=&#34;example-input-file&#34;&gt;Example input file&lt;/h1&gt;
&lt;p&gt;As an input to Logstash, we use a CSV file that contains stock market trades. A few example CSV stock market trades are given below. &lt;/p&gt;</description>
    </item>
    <item>
      <title>Deduplicating documents in Elasticsearch</title>
      <link>http://localhost:1313/posts/deduplicating-documents-in-elasticsearch/</link>
      <pubDate>Mon, 23 Jul 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/deduplicating-documents-in-elasticsearch/</guid>
      <description>&lt;p&gt;This article is available at: &lt;a href=&#34;https://www.elastic.co/blog/how-to-find-and-remove-duplicate-documents-in-elasticsearch&#34;&gt;https://www.elastic.co/blog/how-to-find-and-remove-duplicate-documents-in-elasticsearch&lt;/a&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>

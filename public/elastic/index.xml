<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Elastic Stack on Alexander Marquardt</title>
    <link>http://localhost:1313/elastic/</link>
    <description>Recent content in Elastic Stack on Alexander Marquardt</description>
    <generator>Hugo -- 0.153.4</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 29 Oct 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/elastic/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Ranking by Profit and Popularity in Elasticsearch</title>
      <link>http://localhost:1313/elastic/ranking-by-profit-and-popularity-in-elasticsearch/</link>
      <pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/elastic/ranking-by-profit-and-popularity-in-elasticsearch/</guid>
      <description>&lt;p&gt;Moved to Elastic&amp;rsquo;s blog - &lt;a href=&#34;https://www.elastic.co/search-labs/blog/function-score-query-boosting-profit-popularity-elasticsearch&#34;&gt;https://www.elastic.co/search-labs/blog/function-score-query-boosting-profit-popularity-elasticsearch&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Personalizing e-commerce search results based on purchase history in Elasticsearch (Without a need for Machine Learning Post Processing)</title>
      <link>http://localhost:1313/elastic/personalizing-e-commerce-search-results-based-on-purchase-history-in-elasticsearch-without-a-need-for-machine-learning-post-processing/</link>
      <pubDate>Fri, 12 Sep 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/elastic/personalizing-e-commerce-search-results-based-on-purchase-history-in-elasticsearch-without-a-need-for-machine-learning-post-processing/</guid>
      <description>&lt;p&gt;Whether you’re looking for a product in an online store, an article in a news archive, or a file in a company knowledge base, the quality of the search experience determines how quickly you find what you need. Behind the scenes, many of these systems are powered by &lt;strong&gt;Elasticsearch&lt;/strong&gt;, a popular open-source search engine designed to handle large volumes of data and return relevant results in milliseconds.&lt;/p&gt;
&lt;p&gt;At its core, Elasticsearch matches user queries against text fields and ranks results using relevance scoring. But search doesn’t have to stop there. That’s where &lt;strong&gt;personalization&lt;/strong&gt; comes in. By incorporating signals such as past purchases, browsing behavior, or recent activity, search results can be adjusted so the items most relevant to &lt;em&gt;you&lt;/em&gt; appear higher. For example, if two people both search for &lt;em&gt;“chips”&lt;/em&gt;, one might see &lt;em&gt;classic potato chips&lt;/em&gt; at the top, while the other sees &lt;em&gt;crispy thin-cut chips&lt;/em&gt;, depending on their history.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Efficient bitwise matching of documents in Elasticsearch</title>
      <link>http://localhost:1313/elastic/efficient-bitwise-matching-of-documents-in-elasticsearch/</link>
      <pubDate>Sat, 15 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/elastic/efficient-bitwise-matching-of-documents-in-elasticsearch/</guid>
      <description>&lt;p&gt;This article has now been published on Elastic&amp;rsquo;s website. Please check it out at: &lt;a href=&#34;https://www.elastic.co/search-labs/blog/efficient-bitwise-matching-in-elasticsearch&#34;&gt;https://www.elastic.co/search-labs/blog/efficient-bitwise-matching-in-elasticsearch&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Automating the Import and Export of Kibana Saved Objects</title>
      <link>http://localhost:1313/elastic/automating-the-import-and-export-of-kibana-saved-objects/</link>
      <pubDate>Fri, 03 May 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/elastic/automating-the-import-and-export-of-kibana-saved-objects/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/kibana/current/introduction.html&#34;&gt;Kibana&lt;/a&gt; is an open-source data visualization and exploration tool used for log and time-series analytics, application monitoring, and operational intelligence use cases. It offers powerful and easy-to-use features that allow users to visualize data from Elasticsearch in various formats such as charts, tables, and maps.&lt;/p&gt;
&lt;p&gt;While Kibana offers a robust user interface for managing many tasks, certain operations can become tedious and time-consuming when done manually, especially for operations teams managing large and complex environments. One such operation is the migration of Kibana spaces and objects between environments—a task that can be critical in scenarios where clients cannot utilize the &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/snapshot-restore.html&#34;&gt;snapshot/restore functionality&lt;/a&gt; provided by Elasticsearch.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Re-directing Elasticsearch documents with out-of-range timestamps that (would) fail to get written into Time Series Data Streams</title>
      <link>http://localhost:1313/elastic/re-directing-elasticsearch-documents-with-out-of-range-timestamps-that-would-fail-to-get-written-into-time-series-data-streams/</link>
      <pubDate>Tue, 16 Apr 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/elastic/re-directing-elasticsearch-documents-with-out-of-range-timestamps-that-would-fail-to-get-written-into-time-series-data-streams/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Elasticsearch &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/tsds.html&#34;&gt;Time Series Data Streams (TSDS)&lt;/a&gt; are designed to provide an efficient and scalable way to handle time-based data within the Elasticsearch ecosystem. This feature is specifically optimized for storing, searching, and managing time-series data such as metrics, and events, where data is continuously indexed in chronological order. However, if events arrive with timestamps that fall outside of a pre-defined range, they will be lost.&lt;/p&gt;
&lt;p&gt;In this blog I will demonstrate logic that can be added to an Elasticsearch ingest pipeline which can be used to intercept documents that would be rejected by the TSDS index due to timestamp range issues, and to instead redirect them to a &amp;ldquo;failed&amp;rdquo; index. The documents that are redirected to the &amp;ldquo;failed&amp;rdquo; index may (for example) be used to raise alerts and examined.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Using Logstash to scan inside event contents to replace sensitive data with a consistent hash</title>
      <link>http://localhost:1313/elastic/using-logstash-to-scan-inside-event-contents-to-replace-sensitive-data-with-a-consistent-hash/</link>
      <pubDate>Thu, 20 Jan 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/elastic/using-logstash-to-scan-inside-event-contents-to-replace-sensitive-data-with-a-consistent-hash/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/logstash/&#34;&gt;Logstash&lt;/a&gt; is commonly used for transforming data before it is sent to another system for storage, and so it is often well positioned for finding and replacing sensitive text, as may be required for GDPR compliance.&lt;/p&gt;
&lt;p&gt;Therefore, in this blog I show how Logstash can make use of a ruby filter to scan through the contents of an event and to replace &lt;em&gt;each occurrence&lt;/em&gt; of sensitive text with the value of its hash.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Combining Elasticsearch stemmers and synonyms to improve search relevance</title>
      <link>http://localhost:1313/elastic/combining-elasticsearch-stemmers-and-synonyms-to-improve-search-relevance/</link>
      <pubDate>Sat, 15 May 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/elastic/combining-elasticsearch-stemmers-and-synonyms-to-improve-search-relevance/</guid>
      <description>&lt;p&gt;This is now published on Elastic&amp;rsquo;s official blog. Please check it out at: &lt;a href=&#34;https://www.elastic.co/blog/improve-search-relevance-by-combining-elasticsearch-stemmers-and-synonyms&#34;&gt;https://www.elastic.co/blog/improve-search-relevance-by-combining-elasticsearch-stemmers-and-synonyms&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Driving Filebeat data into separate indices (uses legacy index templates)</title>
      <link>http://localhost:1313/elastic/driving-filebeat-data-into-separate-indices-uses-legacy-index-templates/</link>
      <pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/elastic/driving-filebeat-data-into-separate-indices-uses-legacy-index-templates/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;When driving data into &lt;a href=&#34;https://www.elastic.co/elasticsearch/&#34;&gt;Elasticsearch&lt;/a&gt; from &lt;a href=&#34;https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-overview.html&#34;&gt;Filebeat&lt;/a&gt;, the default behaviour is for all data to be sent into the same destination index regardless of the source of the data. This may not always be desirable since data from different sources may have different access requirements , different retention policies, or different ingest processing requirements.&lt;/p&gt;
&lt;p&gt;In this post, we&amp;rsquo;ll use Filebeat to send data from separate sources into multiple indices, and then we&amp;rsquo;ll use &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/index-lifecycle-management.html&#34;&gt;index lifecycle management (ILM)&lt;/a&gt;, &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/7.11/indices-templates-v1.html&#34;&gt;&lt;em&gt;legacy&lt;/em&gt; index templates&lt;/a&gt;, and a &lt;a href=&#34;https://www.elastic.co/blog/new-way-to-ingest-part-1&#34;&gt;custom ingest pipeline&lt;/a&gt; to further control that data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Using Kibana&#39;s Painless Lab (Beta) to test an ingest processor script</title>
      <link>http://localhost:1313/elastic/using-kibanas-painless-lab-beta-to-test-an-ingest-processor-script/</link>
      <pubDate>Mon, 09 Nov 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/elastic/using-kibanas-painless-lab-beta-to-test-an-ingest-processor-script/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In several previous blog posts I have shown how a Painless script can be used to process new documents as they are ingested into an Elasticsearch cluster. In each of these posts I have made use of the simulate pipeline API to test the Painless scripts.&lt;/p&gt;
&lt;p&gt;While developing such scripts, it may be helpful to use &lt;a href=&#34;https://www.elastic.co/guide/en/kibana/current/painlesslab.html&#34;&gt;Painless Lab&lt;/a&gt; (Beta) in Kibana to debug Painless scripts. In this blog I will show how to use Painless Lab to develop and debug custom scripts, and then show how these can be then easily copied into ingest pipelines.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Using Elasticsearch Painless scripting to recursively iterate through JSON fields</title>
      <link>http://localhost:1313/elastic/using-elasticsearch-painless-scripting-to-iterate-through-fields/</link>
      <pubDate>Fri, 06 Nov 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/elastic/using-elasticsearch-painless-scripting-to-iterate-through-fields/</guid>
      <description>&lt;h2 id=&#34;authors&#34;&gt;Authors&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Alexander Marquardt&lt;/li&gt;
&lt;li&gt;Honza Kral&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-scripting-painless.html&#34;&gt;Painless&lt;/a&gt;&lt;/em&gt; is a simple, secure scripting language designed specifically for use with Elasticsearch. It is the default scripting language for Elasticsearch and can safely be used for inline and stored scripts. In one of its many use cases, Painless can modify documents as they are ingested into your Elasticsearch cluster. In this use case, you may find that you would like to use Painless to evaluate every field in each document that is received by Elasticsearch. However, because of the hierarchical nature of JSON documents, how to iterate over all of the fields may be non-obvious.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Understanding and fixing &#34;too many script compilations&#34; errors in Elasticsearch</title>
      <link>http://localhost:1313/elastic/elasticsearch-too-many-script-compilations/</link>
      <pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/elastic/elasticsearch-too-many-script-compilations/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;When using Elasticsearch, in some rare instances you may see an error such as &amp;ldquo;Too many dynamic script compilations within X minutes&amp;rdquo;. Such an error may be caused by a poor script design &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-scripting-using.html#prefer-params&#34;&gt;where parameters are hard-coded&lt;/a&gt;. In other cases this may be due to the script cache being too small or the compilation limit being too low. In this article, I will show how to determine if these default limits are too low, and how these limits can be modified.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Using Logstash and Elasticsearch to calculate transaction duration in a microservices architecture</title>
      <link>http://localhost:1313/elastic/using-logstash-and-elasticsearch-scripted-upserts-to-calculate-transaction-duration-from-out-of-order-events/</link>
      <pubDate>Wed, 16 Sep 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/elastic/using-logstash-and-elasticsearch-scripted-upserts-to-calculate-transaction-duration-from-out-of-order-events/</guid>
      <description>&lt;p&gt;September 16, 2020&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Elasticsearch  allows you to unify your observability data in a powerful datastore so you can search and apply interactive analytics in real time to a huge number of use cases.&lt;/p&gt;
&lt;p&gt;In one such use case, you may be using Elasticsearch to monitor a system that is composed of multiple microservices that process a given transaction. For such a system, you may be collecting an event corresponding to when the first microservice in the system starts processing the transaction, and another event corresponding to when the last microservice in the system finishes processing the transaction. In such an approach, each event should include a field with the transaction identifier, which will allow multiple events corresponding to a single transaction to be combined for analysis.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Using Grok with Elasticsearch to add structure to your data</title>
      <link>http://localhost:1313/elastic/using-grok-with-elasticsearch-to-add-structure-to-your-data/</link>
      <pubDate>Mon, 13 Jul 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/elastic/using-grok-with-elasticsearch-to-add-structure-to-your-data/</guid>
      <description>&lt;p&gt;This article is available on Elastic&amp;rsquo;s blog as a 3-part series. Please check it out at the following URLs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/blog/structuring-elasticsearch-data-with-grok-on-ingest-for-faster-analytics&#34;&gt;https://www.elastic.co/blog/structuring-elasticsearch-data-with-grok-on-ingest-for-faster-analytics&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/blog/slow-and-steady-how-to-build-custom-grok-patterns-incrementally&#34;&gt;https://www.elastic.co/blog/slow-and-steady-how-to-build-custom-grok-patterns-incrementally&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/blog/debugging-broken-grok-expressions-in-elasticsearch-ingest-processors&#34;&gt;https://www.elastic.co/blog/debugging-broken-grok-expressions-in-elasticsearch-ingest-processors&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Storing ingest time and calculating ingest lag in Elasticsearch</title>
      <link>http://localhost:1313/elastic/storing-ingest-time-and-calculating-ingest-lag-in-elasticsearch/</link>
      <pubDate>Tue, 02 Jun 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/elastic/storing-ingest-time-and-calculating-ingest-lag-in-elasticsearch/</guid>
      <description>&lt;p&gt;This article is available on Elastic&amp;rsquo;s blog at: &lt;a href=&#34;https://www.elastic.co/blog/calculating-ingest-lag-and-storing-ingest-time-in-elasticsearch-to-improve-observability&#34;&gt;https://www.elastic.co/blog/calculating-ingest-lag-and-storing-ingest-time-in-elasticsearch-to-improve-observability&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Using boolean queries to improve search relevancy in Elasticsearch</title>
      <link>http://localhost:1313/elastic/using-boolean-queries-to-improve-search-relevancy-in-elasticsearch/</link>
      <pubDate>Tue, 12 May 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/elastic/using-boolean-queries-to-improve-search-relevancy-in-elasticsearch/</guid>
      <description>&lt;p&gt;Page moved to &lt;a href=&#34;https://alexmarquardt.com/using-boolean-queries-to-improve-search-relevancy-in-elasticsearch/&#34;&gt;https://alexmarquardt.com/using-boolean-queries-to-improve-search-relevancy-in-elasticsearch/&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Using boolean queries to improve search relevance in Elasticsearch</title>
      <link>http://localhost:1313/elastic/using-boolean-queries-to-improve-search-relevance-in-elasticsearch/</link>
      <pubDate>Fri, 01 May 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/elastic/using-boolean-queries-to-improve-search-relevance-in-elasticsearch/</guid>
      <description>&lt;p&gt;This article is available at: &lt;a href=&#34;https://www.elastic.co/blog/how-to-improve-elasticsearch-search-relevance-with-boolean-queries&#34;&gt;https://www.elastic.co/blog/how-to-improve-elasticsearch-search-relevance-with-boolean-queries&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Using slow logs in Elastic Cloud Enterprise</title>
      <link>http://localhost:1313/elastic/using-slow-logs-in-elastic-cloud-enterprise/</link>
      <pubDate>Sun, 26 Apr 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/elastic/using-slow-logs-in-elastic-cloud-enterprise/</guid>
      <description>&lt;p&gt;April 26, 2020&lt;/p&gt;
&lt;h1 id=&#34;authors&#34;&gt;Authors&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Tom Schreiber&lt;/li&gt;
&lt;li&gt;Alex Marquardt&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;version&#34;&gt;Version&lt;/h1&gt;
&lt;p&gt;This blog article is based on ECE 2.4.3.&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/cloud-enterprise/2.4/Elastic-Cloud-Enterprise-overview.html&#34;&gt;Elastic Cloud Enterprise&lt;/a&gt; (ECE) is a platform designed to ease the management, deployment, and configuration of multiple Elasticsearch clusters through a single administrative user interface. ECE, is the same product that powers the &lt;a href=&#34;https://www.elastic.co/elasticsearch/service&#34;&gt;Elasticsearch Service&lt;/a&gt; hosted offering, and is available for installation on customer-managed servers. ECE can be deployed anywhere - on public or private clouds, virtual machines, or even on bare metal hardware. Once installed, ECE allows Elasticsearch clusters to be created, upgraded, or deleted with the click of a button.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Using Elastic machine learning to detect anomalies in derivative values</title>
      <link>http://localhost:1313/elastic/using-elastic-machine-learning-to-detect-anomalies-in-derivative-values/</link>
      <pubDate>Tue, 21 Apr 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/elastic/using-elastic-machine-learning-to-detect-anomalies-in-derivative-values/</guid>
      <description>&lt;p&gt;April 21, 2020&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;In this blog, we use Elastic machine learning (ML) and derivative aggregations to detect sudden unexpected increases or decreases in the &lt;em&gt;rate-of-change&lt;/em&gt; of CPU load on servers that are monitored by &lt;a href=&#34;https://www.elastic.co/guide/en/beats/metricbeat/7.6/metricbeat-overview.html&#34;&gt;Metricbeat&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In order to make this blog easier to follow and the results easy to recreate, we abstract away the requirement for driving data from Metricbeat, and instead generate &amp;ldquo;fake&amp;rdquo; Metricbeat data using a Python script that drives data into Elasticsearch.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Using the Elasticsearch Enrich Processor with CSV data</title>
      <link>http://localhost:1313/elastic/using-the-elasticsearch-enrich-processor-with-csv-data/</link>
      <pubDate>Sat, 21 Mar 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/elastic/using-the-elasticsearch-enrich-processor-with-csv-data/</guid>
      <description>&lt;p&gt;This article is available at: &lt;a href=&#34;https://www.elastic.co/blog/how-to-enrich-logs-and-metrics-using-an-elasticsearch-ingest-node&#34;&gt;https://www.elastic.co/blog/how-to-enrich-logs-and-metrics-using-an-elasticsearch-ingest-node&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Converting CSV to JSON in Filebeat</title>
      <link>http://localhost:1313/elastic/converting-csv-to-json-in-filebeat/</link>
      <pubDate>Tue, 17 Mar 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/elastic/converting-csv-to-json-in-filebeat/</guid>
      <description>&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Many organisations use excel files for creating and storing important data. For various reasons it may be useful to import such data into Elasticsearch. For example, one may need to get &lt;a href=&#34;https://en.wikipedia.org/wiki/Master_data&#34;&gt;Master Data&lt;/a&gt; that is created in a spreadsheet into Elasticsearch where it could be used for &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/7.x/ingest-enriching-data.html&#34;&gt;enriching Elasticsearch documents&lt;/a&gt;. Or one may wish to use Elasticsearch and Kibana for analysing a dataset that is only available in a spreadsheet. In such cases, one option is to use &lt;a href=&#34;https://www.elastic.co/guide/en/beats/filebeat/7.6/filebeat-overview.html&#34;&gt;Filebeat&lt;/a&gt; for uploading such CSV data into an Elasticsearch cluster.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Enriching data with the Logstash translate filter</title>
      <link>http://localhost:1313/elastic/enriching-data-with-the-logstash-translate-filter/</link>
      <pubDate>Fri, 06 Mar 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/elastic/enriching-data-with-the-logstash-translate-filter/</guid>
      <description>&lt;p&gt;March 6, 2020&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/logstash&#34;&gt;Logstash&lt;/a&gt; is an open source, server-side data processing pipeline that ingests data from a multitude of sources, transforms it, and then sends it to one or more outputs. One use of Logstash is for enriching data before sending it to Elasticsearch.&lt;/p&gt;
&lt;p&gt;Logstash supports several different &lt;a href=&#34;https://www.elastic.co/guide/en/logstash/current/lookup-enrichment.html#geoip-def&#34;&gt;lookup plugin filters&lt;/a&gt; that can be used for enriching data. Many of these rely on components that are external to the Logstash pipeline for storing enrichment data. On the other hand, the &lt;a href=&#34;https://www.elastic.co/guide/en/logstash/current/plugins-filters-translate.html&#34;&gt;translate filter plugin&lt;/a&gt; can be used for looking up data and enriching documents without dependencies. Therefore, in this blog article I focus on using Logstash with the translate filter plugin for enriching data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to create maintainable and reusable logstash pipelines</title>
      <link>http://localhost:1313/elastic/how-to-create-maintainable-and-reusable-logstash-pipelines/</link>
      <pubDate>Wed, 26 Feb 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/elastic/how-to-create-maintainable-and-reusable-logstash-pipelines/</guid>
      <description>&lt;p&gt;This article is available at: &lt;a href=&#34;https://www.elastic.co/blog/how-to-create-maintainable-and-reusable-logstash-pipelines&#34;&gt;https://www.elastic.co/blog/how-to-create-maintainable-and-reusable-logstash-pipelines&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Using Logstash and Elasticsearch scripted upserts to transform eCommerce purchasing data</title>
      <link>http://localhost:1313/elastic/logstash-and-elasticsearch-painless-scripted-upserts-transform-data/</link>
      <pubDate>Tue, 17 Dec 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/elastic/logstash-and-elasticsearch-painless-scripted-upserts-transform-data/</guid>
      <description>&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/logstash/7.5/introduction.html&#34;&gt;Logstash&lt;/a&gt; is a tool that can be used to collect, process, and forward events to Elasticsearch. In order to demonstrate the power of Logstash when used in conjunction with Elasticsearch’s &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/7.5/docs-update.html#scripted_upsert&#34;&gt;scripted upserts&lt;/a&gt;, I will show you how to create a near-real-time &lt;a href=&#34;https://www.elastic.co/elasticon/2015/sf/building-entity-centric-indexes&#34;&gt;entity-centric index&lt;/a&gt;. Once data is transformed into an entity-centric index, many kinds of analysis become possible with simple (cheap) queries rather than more computationally intensive aggregations.&lt;/p&gt;
&lt;p&gt;As a note, using the approach demonstrated here would result in documents similar to those generated by &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/7.5/transforms.html&#34;&gt;Elasticsearch transforms&lt;/a&gt;. Nevertheless, the technique that is documented has not been benchmarked against Elasticsearch transforms, as the main goal of this blog is to demonstrate the power and flexibility of Logstash combined with scripted upserts.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Emulating transactional functionality in Elasticsearch with two-phase commits</title>
      <link>http://localhost:1313/elastic/emulating-transactional-functionality-in-elasticsearch-with-two-phase-commits/</link>
      <pubDate>Thu, 05 Dec 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/elastic/emulating-transactional-functionality-in-elasticsearch-with-two-phase-commits/</guid>
      <description>&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Elasticsearch supports atomic create, update, and delete operations at the individual document level, but does not have built-in &lt;a href=&#34;https://www.elastic.co/blog/found-elasticsearch-as-nosql#transactions&#34;&gt;support for multi-document transactions&lt;/a&gt;. Although Elasticsearch does not position itself as a system of record for storing data, in some cases it may be necessary to modify multiple documents as a single cohesive unit. Therefore, in this blog post we present a &lt;a href=&#34;https://en.wikipedia.org/wiki/Two-phase_commit_protocol&#34;&gt;two-phase commit protocol&lt;/a&gt; which can be used to &lt;em&gt;emulate&lt;/em&gt; multi-document transactions.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Converting local time to ISO 8601 time in Elasticsearch</title>
      <link>http://localhost:1313/elastic/converting-local-time-to-iso-8601-time-in-elasticsearch/</link>
      <pubDate>Wed, 16 Oct 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/elastic/converting-local-time-to-iso-8601-time-in-elasticsearch/</guid>
      <description>&lt;p&gt;This article is available at: &lt;a href=&#34;https://www.elastic.co/blog/converting-local-time-to-iso-8601-time-in-elasticsearch&#34;&gt;https://www.elastic.co/blog/converting-local-time-to-iso-8601-time-in-elasticsearch&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Es Local Indexer - using Elasticsearch for searching locally stored documents</title>
      <link>http://localhost:1313/elastic/es-local-indexer-using-elasticsearch-for-searching-locally-stored-documents/</link>
      <pubDate>Thu, 08 Aug 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/elastic/es-local-indexer-using-elasticsearch-for-searching-locally-stored-documents/</guid>
      <description>&lt;p&gt;Moved to: &lt;a href=&#34;https://alexmarquardt.com/es-local-indexer-desktop-search-built-with-elasticsearch/&#34;&gt;https://alexmarquardt.com/es-local-indexer-desktop-search-built-with-elasticsearch/&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>ES Local Indexer – Desktop search powered by Elasticsearch</title>
      <link>http://localhost:1313/elastic/es-local-indexer-desktop-search-built-with-elasticsearch/</link>
      <pubDate>Thu, 08 Aug 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/elastic/es-local-indexer-desktop-search-built-with-elasticsearch/</guid>
      <description>&lt;p&gt;August 7, 2019&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Elasticsearch provides search functionality for some of the most important websites in the world including &lt;a href=&#34;https://blog.wikimedia.org/2014/01/06/wikimedia-moving-to-elasticsearch/&#34;&gt;Wikimedia (i.e. Wikipedia)&lt;/a&gt;, &lt;a href=&#34;https://www.elastic.co/videos/ebay-and-elasticsearch-this-is-not-small-data&#34;&gt;eBay&lt;/a&gt;, &lt;a href=&#34;https://engineeringblog.yelp.com/2017/06/moving-yelps-core-business-search-to-elasticsearch.html&#34;&gt;Yelp&lt;/a&gt;, &lt;a href=&#34;https://www.elastic.co/elasticon/conf/2017/sf/tinder-using-the-elastic-stack-to-make-connections-around-the-world&#34;&gt;Tinder&lt;/a&gt;, and many others. Elasticsearch is super scalable, which means that just as easily as it can be scaled it up for use in huge complex systems, it can also be scaled down for use in smaller projects.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/alexander-marquardt/es_local_indexer&#34;&gt;ES Local Indexer&lt;/a&gt; is a small desktop search application that runs on top of a local Elasticsearch installation. It indexes HTML documents into Elasticsearch and provides an intuitive browser-based interface for searching through the ingested documents. The ES Local Indexer project consists of two main components:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Counting unique beats agents sending data into Elasticsearch</title>
      <link>http://localhost:1313/elastic/counting-unique-beats-agents-elasticsearch/</link>
      <pubDate>Thu, 18 Jul 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/elastic/counting-unique-beats-agents-elasticsearch/</guid>
      <description>&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;When using Beats with Elasticsearch, it may be useful to keep track of how many unique agents are sending data into an Elasticsearch cluster, and how many documents each agent is submitting. Such information for example could be useful for detecting if beats agents are behaving as expected.&lt;/p&gt;
&lt;p&gt;In this blog post, I first discuss how to efficiently specify a filter for documents corresponding to a particular time range, followed by several methods for detecting how many beats agents are sending documents to Elasticsearch within the specified time range.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Improving the performance of Logstash persistent queues</title>
      <link>http://localhost:1313/elastic/improving-the-performance-of-logstash-persistent-queues/</link>
      <pubDate>Sat, 15 Jun 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/elastic/improving-the-performance-of-logstash-persistent-queues/</guid>
      <description>&lt;p&gt;This article is available at: &lt;a href=&#34;https://www.elastic.co/blog/using-parallel-logstash-pipelines-to-improve-persistent-queue-performance&#34;&gt;https://www.elastic.co/blog/using-parallel-logstash-pipelines-to-improve-persistent-queue-performance&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Debugging Elasticsearch and Lucene with IntelliJ IDEA</title>
      <link>http://localhost:1313/elastic/debugging-elasticsearch-and-lucene-with-intellij-idea/</link>
      <pubDate>Sat, 02 Feb 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/elastic/debugging-elasticsearch-and-lucene-with-intellij-idea/</guid>
      <description>&lt;p&gt;This article can be found at: &lt;a href=&#34;https://www.elastic.co/blog/how-to-debug-elasticsearch-source-code-in-intellij-idea&#34;&gt;https://www.elastic.co/blog/how-to-debug-elasticsearch-source-code-in-intellij-idea&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>A step-by-step guide to enabling security, TLS/SSL, and PKI authentication in Elasticsearch</title>
      <link>http://localhost:1313/elastic/a-step-by-step-guide-to-enabling-security-tls/ssl-and-pki-authentication-in-elasticsearch/</link>
      <pubDate>Mon, 05 Nov 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/elastic/a-step-by-step-guide-to-enabling-security-tls/ssl-and-pki-authentication-in-elasticsearch/</guid>
      <description>&lt;p&gt;This article is available at: &lt;a href=&#34;https://www.elastic.co/blog/elasticsearch-security-configure-tls-ssl-pki-authentication&#34;&gt;https://www.elastic.co/blog/elasticsearch-security-configure-tls-ssl-pki-authentication&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to tune Elasticsearch for aggregation performance</title>
      <link>http://localhost:1313/elastic/how-to-tune-elasticsearch-for-aggregation-performance/</link>
      <pubDate>Tue, 02 Oct 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/elastic/how-to-tune-elasticsearch-for-aggregation-performance/</guid>
      <description>&lt;p&gt;October 2, 2018&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;By default, Elasticsearch is tuned for the best trade-off between write performance and query performance for the majority of use cases. In this blog posting we cover some parameters that can be configured to improve query-time aggregation performance, with &lt;em&gt;some&lt;/em&gt; of these improvements coming at &lt;em&gt;the expense of write performance.&lt;/em&gt; &lt;/p&gt;
&lt;p&gt;Note that this blog posting does not present anything that is not already documented in other locations. The goal here is to pull together relevant information into a small and digestible posting that provides a few pointers on how to improve slow Elasticsearch aggregations.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Using Logstash to drive filtered data from a single source into multiple output destinations</title>
      <link>http://localhost:1313/elastic/using-logstash-to-drive-filtered-data-from-a-single-source-into-multiple-output-destinations/</link>
      <pubDate>Fri, 31 Aug 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/elastic/using-logstash-to-drive-filtered-data-from-a-single-source-into-multiple-output-destinations/</guid>
      <description>&lt;p&gt;This article is available at: &lt;a href=&#34;https://www.elastic.co/blog/using-logstash-to-split-data-and-send-it-to-multiple-outputs&#34;&gt;https://www.elastic.co/blog/using-logstash-to-split-data-and-send-it-to-multiple-outputs&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Using Logstash prune capabilities to whitelist sub-documents</title>
      <link>http://localhost:1313/elastic/using-logstash-prune-capabilities-to-whitelist-sub-documents/</link>
      <pubDate>Tue, 28 Aug 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/elastic/using-logstash-prune-capabilities-to-whitelist-sub-documents/</guid>
      <description>&lt;h1 id=&#34;overview&#34;&gt;Overview&lt;/h1&gt;
&lt;p&gt;Logstash&amp;rsquo;s &lt;a href=&#34;https://www.elastic.co/guide/en/logstash/current/plugins-filters-prune.html&#34;&gt;prune filter plugin&lt;/a&gt; can make use of whitelists to ensure that only specific desired fields are output from Logstash, and that all other fields are dropped. In this blog post we demonstrate the use of Logstash to whitelist desired fields and desired sub-documents before indexing into Elasticsearch.&lt;/p&gt;
&lt;h1 id=&#34;example-input-file&#34;&gt;Example input file&lt;/h1&gt;
&lt;p&gt;As an input to Logstash, we use a CSV file that contains stock market trades. A few example CSV stock market trades are given below. &lt;/p&gt;</description>
    </item>
    <item>
      <title>Deduplicating documents in Elasticsearch</title>
      <link>http://localhost:1313/elastic/deduplicating-documents-in-elasticsearch/</link>
      <pubDate>Mon, 23 Jul 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/elastic/deduplicating-documents-in-elasticsearch/</guid>
      <description>&lt;p&gt;This article is available at: &lt;a href=&#34;https://www.elastic.co/blog/how-to-find-and-remove-duplicate-documents-in-elasticsearch&#34;&gt;https://www.elastic.co/blog/how-to-find-and-remove-duplicate-documents-in-elasticsearch&lt;/a&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
